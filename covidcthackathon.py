# -*- coding: utf-8 -*-
"""CovidCTHackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DUne5PBKd3H11Xm2PKQw7Tz14JWK-lyb
"""

import nibabel as nib
import matplotlib.pyplot as plt
import glob
import os
import tqdm

# Downloading dataset and unzipping file
import os


# Loading images metadata
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import models
import numpy as np
from PIL import Image
import cv2 
from nibabel.viewers import OrthoSlicer3D
from matplotlib import cm
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision import transforms, datasets
import time
import copy
from tqdm import tqdm, tqdm_notebook
from torch.utils.data.sampler import Sampler
from sklearn.model_selection import train_test_split
from random import choice
import torchmetrics
from sklearn.metrics import roc_curve, confusion_matrix, roc_auc_score, precision_recall_curve, multilabel_confusion_matrix, precision_score
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.multiclass import OneVsRestClassifier

def create_processed_arr(img,root_dir=None):
  """
  Function to convert an image path or buffer into numpy array preprocessed
  # TODO: Define args and return type


  """
  # TODO: Data Augmentation
  # https://www.researchgate.net/post/Data-augmentation-techniques-for-medical-image
  preprocess = transforms.Compose([
  transforms.Resize(224),
  transforms.ToTensor(),
  transforms.Normalize(
    mean=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225],
  ),
  transforms.RandomHorizontalFlip()
  ])
  im = None
  if root_dir:
    im = Image.open(root_dir+img).convert("RGB")
  else:
    im = Image.open(img).convert("RGB")
  fin_array = preprocess(im)
  return fin_array

# TODO; Changing grayscale conversion (without matplotlib): @Yujin
def convert_to_rgb(x,root_dir):
  """
  Converting .nii.gz files into one 2D slice in RGB format
  # TODO: Define args and return type

  """
  nib_img = nib.load(root_dir + "/%s" % x)
  np_array = nib_img.get_fdata()
  img = np_array[:,:,np_array.shape[-1]  // 2]
  # fig = plt.figure(figsize=(224,224))
  # import io
  # buf = io.BytesIO()
  # plt.imsave(buf, img, cmap="gray")
  # buf.seek(0)
  # fin_array = create_processed_arr(buf)
  # buf.close()
  # plt.close(fig)
  return img


class CovidCTDataset(Dataset):
  """
    Args:
        excel_file (string): Path to the excel file with annotations.
        root_dir (string): Directory with all the images.
        transform (callable, optional): Optional transform to be applied
            on a sample.
  """
  def __init__(self, excel_file, root_dir, transform=None):
    tqdm().pandas()
    self.ct_df = pd.read_excel(excel_file)
    # Integer encode CT-0 to 0 and CT-1> to 1
    self.ct_df = self.ct_df[self.ct_df["category"] != "CT-4"]
    self.ct_df["identification"] = pd.Series(np.where(self.ct_df.category.values == 'CT-0', 0, 1),
          self.ct_df.index)
    self.ct_df = self.ct_df.sample(frac=1)
    if "COVID19_RGB" in root_dir:
        self.ct_df["img_arr"] = self.ct_df.study_file.progress_apply(create_processed_arr,args=(root_dir,))
    else:
      self.ct_df["img_arr"] = self.ct_df.study_file.progress_apply(convert_to_rgb,args=(root_dir,))
    self.root_dir = root_dir
    print(self.ct_df["identification"].value_counts())
    self.transform = transform

  def __len__(self):
    return len(self.ct_df)

  def __getitem__(self, idx):
    ct_item = self.ct_df.iloc[idx]
    label = ct_item["identification"]
    fin_array = ct_item["img_arr"]
    result = (fin_array, torch.tensor(label))
    return result

# https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py#L73-L90
# https://gist.github.com/ilkarman/fe45c586ef10c4cb8e52b3a78b6ac854
# Sampler to balance batches between non-COVID and COVID data
class BalancedSampler(Sampler):
  def __init__(self, dataset):
    self.dataset = dict()
    self.indices = range(0,len(dataset))
    self.max_elems = 0
    self.keys = []
    # Iterates through entire dataset to get distribution of classes in dataset
    for index in range(0,len(dataset)):
      # Obtain the label and add the index associated with the label (create an empty list if needed)
      label = dataset[index][1].item()
      if label not in self.dataset:
        self.dataset[label] = list()
        self.keys.append(label)
      self.dataset[label].append(index)
      # Record the max amount of elements assigned to a label
      if len(self.dataset[label]) > self.max_elems:
        self.max_elems = len(self.dataset[label])
    # Sample more elements of undersampled class
    for label in self.dataset:
      while(len(self.dataset[label]) < self.max_elems):
        new_elem = choice(self.dataset[label])
        self.dataset[label].append(new_elem)
    self.currentkey = 0
    self.indices = [-1]*len(self.keys)
    print("Max elems: %d" % self.max_elems)
  def __iter__(self):
    result = []
    while self.indices[self.currentkey] < self.max_elems - 1:
        self.indices[self.currentkey] += 1
        yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]
        self.currentkey = (self.currentkey + 1) % len(self.keys)
    self.indices = [-1]*len(self.keys)


  def __len__(self):
    return self.max_elems*len(self.keys)




class CovidCTDatasetSev(Dataset):
  """
    Args:
        excel_file (string): Path to the excel file with annotations.
        root_dir (string): Directory with all the images.
        transform (callable, optional): Optional transform to be applied
            on a sample.
  """
  def __init__(self, excel_file, root_dir, transform=None):
    tqdm().pandas()
    self.ct_df = pd.read_excel(excel_file)
    # Integer encode CT-0 to 0 and CT-1> to 1
    category_replacements = {"identification": {"CT-0": 0, "CT-1": 1, "CT-2": 2, "CT-3": 3}}
    self.ct_df = self.ct_df[self.ct_df["category"] != "CT-4"]
    self.ct_df["identification"] = self.ct_df["category"]
    self.ct_df = self.ct_df.replace(category_replacements)
          
    self.ct_df = self.ct_df.sample(frac=1)
    if "COVID19_RGB" in root_dir:
        self.ct_df["img_arr"] = self.ct_df.study_file.progress_apply(create_processed_arr,args=(root_dir,))
    else:
      self.ct_df["img_arr"] = self.ct_df.study_file.progress_apply(convert_to_rgb,args=(root_dir,))
    self.root_dir = root_dir
    print(self.ct_df["identification"].value_counts())
    self.transform = transform

  def __len__(self):
    return len(self.ct_df)

  def __getitem__(self, idx):
    ct_item = self.ct_df.iloc[idx]
    label = ct_item["identification"]
    fin_array = ct_item["img_arr"]
    result = (fin_array, torch.tensor(label))
    return result


def main():

  root_dir = ""

  if os.path.exists("/content/drive/MyDrive/COVID19_RGB"):
    root_dir = "/content/drive/MyDrive/COVID19_RGB"

  elif os.path.exists(r"C:\Users\kvikr\Programming\COVID19_RGB"):
    root_dir = r"C:\Users\kvikr\Programming\COVID19_RGB"

  elif os.path.exists("/content/drive/MyDrive/COVID19_1110"):
    root_dir = "/content/drive/MyDrive/COVID19_1110"
    

  elif not os.path.exists("./COVID19_1110"):
    root_dir = "./COVID19_1110"

  print("Root Directory:",root_dir)
  ct_dataset = CovidCTDataset(root_dir+"/dataset_registry.xlsx",root_dir,transform=True)
  targets = [target for feature,target in ct_dataset]

  # Split into train, test set with equal proportion of data in train and test dataset
  # https://linuxtut.com/en/c6023453e00bfead9e9f/
  train_indices, test_indices = train_test_split(list(range(len(ct_dataset))), test_size=0.2, stratify=targets)
  train_dataset = torch.utils.data.Subset(ct_dataset, train_indices)
  targets = [target for feature,target in train_dataset]
  train_indices, val_indices = train_test_split(train_indices, test_size=0.2, stratify=targets)
  train_dataset = torch.utils.data.Subset(ct_dataset, train_indices)
  val_dataset = torch.utils.data.Subset(ct_dataset, val_indices)
  test_dataset = torch.utils.data.Subset(ct_dataset, test_indices)

  balanced_sampler = BalancedSampler(train_dataset)
  #Create DataLoader
  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)
  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)
  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=4)

  # https://discuss.pytorch.org/t/weightedrandomsampler-not-sampling-balanced-batches/89767
  # Discussing balancing imbalanced dataset through stratification in dataset



  # #TODO: Modify to be programmatic
  print(len(balanced_sampler))
  train_set_size = len(train_dataset)
  test_set_size = len(test_dataset)


  # TODO: Train, Val, Test split
  # train_set, test_set = torch.utils.data.random_split(ct_dataset, [train_set_size, len(ct_dataset) - train_set_size])

  # train_dataloader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=0)

  # test_dataloader = DataLoader(test_set, batch_size=16, shuffle=True, num_workers=0)


  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  resnet_model = models.resnet50(pretrained=True)


  # Freeze weights on all layers except the FC layer
  for param in resnet_model.parameters():
    param.requires_grad = False

  # Obtain the number of features the pretrained FC layer has as input
  num_ftrs = resnet_model.fc.in_features

  # Here the size of each output sample is set to 2.
  resnet_model.fc = nn.Linear(num_ftrs, 2)

  # Set the loss function to be Cross Entropy
  criterion = nn.CrossEntropyLoss()

  # Observe that all parameters are being optimized
  optimizer_ft = optim.SGD(resnet_model.parameters(), lr=0.000, momentum=0.9)

  # optimizer_ft = optim.Adam(resnet_model.parameters(), lr=0.001)

  # Decay LR by a factor of 0.1 every 7 epochs
  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.00001)

  # Move model onto device
  # resnet_model = resnet_model.to(device)

  val_losses = []
  train_losses = []
  train_acc = []
  val_acc = []
  dataloaders = {"train": train_dataloader, "val": val_dataloader}
  data_sizes = {"train": train_set_size, "val": len(val_dataset)}

  def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
      since = time.time()
      best_model_wts = copy.deepcopy(model.state_dict())
      best_acc = 0.0
      for epoch in tqdm(range(0,num_epochs)):
        for phase in ['train', 'val']:
          if phase == 'train':
              model.train()  # Set model to training mode
          else:
              model.eval()   # Set model to evaluate mode

          running_loss = 0.0
          running_corrects = 0
        # Setting model to training mode
          model.train()

  
            # Iterate over training data.
          for inputs in dataloaders[phase]:

              inputs_dev = inputs[0].to(device)
              labels_dev = inputs[1].to(device)
              # print(labels_dev)
              # zero the parameter gradients
              optimizer.zero_grad()

              # forward
              # track history
              with torch.set_grad_enabled(phase=="train"):
                  outputs = model(inputs_dev)
                  _, preds = torch.max(outputs, 1)
                  labels_dev = labels_dev.long()
                  loss = criterion(outputs, labels_dev)
                  if phase == 'train':
                    # backward + optimize
                    loss.backward()
                    optimizer.step()

              # statistics
              running_loss += loss.item() * inputs[0].size(0)
              running_corrects += (preds == labels_dev).sum().item()
              # scheduler.step()
          epoch_loss = running_loss / data_sizes[phase]
          epoch_acc = float(running_corrects) / data_sizes[phase]
          print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                  phase, epoch_loss, epoch_acc))

          # deep copy the model
          if phase == 'val':
              val_losses.append(epoch_loss)
              val_acc.append(epoch_acc)
              if epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
          elif phase == 'train':
              train_acc.append(epoch_acc)
              train_losses.append(epoch_loss)

        print()

      time_elapsed = time.time() - since
      print('Training complete in {:.0f}m {:.0f}s'.format(
          time_elapsed // 60, time_elapsed % 60))
      print('Best val Acc: {:4f}'.format(best_acc))

      # load best model weights
      model.load_state_dict(best_model_wts)
      return model


  ct_dataset = CovidCTDatasetSev(root_dir+"/dataset_registry.xlsx",root_dir,transform=True)
  targets = [target for feature,target in ct_dataset]

  # Split into train, test set with equal proportion of data in train and test dataset
  # https://linuxtut.com/en/c6023453e00bfead9e9f/
  train_indices, test_indices = train_test_split(list(range(len(ct_dataset))), test_size=0.2, stratify=targets)
  train_dataset = torch.utils.data.Subset(ct_dataset, train_indices)
  targets = [target for feature,target in train_dataset]

  # Split into train, test set with equal proportion of data in train and test dataset
  # https://linuxtut.com/en/c6023453e00bfead9e9f/
  train_indices, val_indices = train_test_split(train_indices, test_size=0.2, stratify=targets)
  train_dataset = torch.utils.data.Subset(ct_dataset, train_indices)
  val_dataset = torch.utils.data.Subset(ct_dataset, val_indices)
  test_dataset = torch.utils.data.Subset(ct_dataset, test_indices)
  testing_dist = {0:0,1:0,2:0,3:0}
  for i in test_dataset:
    testing_dist[i[1].item()] += 1
  # balanced_sampler = BalancedSampler(train_dataset)
  print(testing_dist)
  #Create DataLoader
  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)
  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)
  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=4)


  # # https://discuss.pytorch.org/t/weightedrandomsampler-not-sampling-balanced-batches/89767
  # # Discussing balancing imbalanced dataset through stratification in dataset



  # # #TODO: Modify to be programmatic
  train_set_size = len(train_dataset)
  test_set_size = len(test_dataset)



  # classes = ("Non-COVID", "COVID")

  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  resnet_model = models.resnet50(pretrained=True)
  # vgg_model = models.vgg16(pretrained=True)
  # densenet_model = models.densenet161(pretrained=True)
  num_params = 0
  # Freeze weights on all layers except the FC layer
  for param in resnet_model.parameters():
    num_params += 1
    param.requires_grad = False
  # curr_param = 0
  # for param in resnet_model.parameters():
  #   if curr_param > num_params - 5:
  #     param.requires_grad = True
  #   curr_param += 1
  print("------")
  # Obtain the number of features the pretrained FC layer has as input
  num_ftrs = resnet_model.fc.in_features
  # num_ftrs_dense = densenet_model.classifier.in_features

  # Here the size of each output sample is set to 4.
  resnet_model.fc = nn.Linear(num_ftrs, 4)
  # vgg_model.classifier[6] = nn.Linear(4096,4)
  # densenet_model.classifier = nn.Linear(num_ftrs_dense, 4)

  # Set the loss function to be Cross Entropy
  criterion = nn.CrossEntropyLoss()

  # Observe that all parameters are being optimized
  optimizer_ft = optim.SGD(resnet_model.parameters(), lr=0.0008, momentum=0.9)


  # Decay LR by a factor of 0.1 every 7 epochs
  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.00001)



  # # From PyTorch Tutorial https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html with modifications
  val_losses = []
  train_losses = []
  train_acc = []
  val_acc = []
  dataloaders = {"train": train_dataloader, "val": val_dataloader}
  data_sizes = {"train": train_set_size, "val": len(val_dataset)}

# # Train the model via the train_model function
  # # Move model onto device
  resnet_model = resnet_model.to(device)
  # vgg_model = vgg_model.to(device)
  # vgg_model = vgg_model.to(device)
  model_ft = train_model(resnet_model, criterion, optimizer_ft, exp_lr_scheduler,
                        num_epochs=20)

  # model_ft_resnet = train_model(resnet_model, criterion, optimizer_ft, exp_lr_scheduler,
  #                       num_epochs=40)




  model_ft.eval()
  correct = 0
  total = 0
  torch.no_grad()
  labels_arr = np.array([])
  preds_arr = np.array([])
  probs_final = dict({0:[],1:[],2:[],3:[]})
  running_corrects = 0
  total = 0
  for inputs in test_dataloader:
    inputs_dev = inputs[0].to(device)
    labels_dev = inputs[1].to(device)
    outputs = model_ft(inputs_dev)
    probs = F.softmax(outputs,dim=1)
    _, preds = torch.max(outputs, 1)

    for i in probs:
      for num in range(0,4):
        prob = i.detach().cpu().numpy()[num]
        # print(max_prob.shape)
        probs_arr = probs_final[num]
        probs_arr.append(prob)
        probs_final[num] = probs_arr
      max_prob = np.max(i.detach().cpu().numpy())
      # probs_final = np.append(probs_final,max_prob)
    labels_arr = np.concatenate((labels_arr,labels_dev.cpu().numpy()))
    preds_arr = np.concatenate((preds_arr,preds.cpu().numpy()))
    running_corrects += (preds == labels_dev).sum().item()
    total += len(preds)
  # con_matrix = confusion_matrix(labels_arr, preds_arr)
  # TODO: Postprocess to create ROC curve @Yuyan
  # https://torchmetrics.readthedocs.io/en/latest/references/modules.html#auroc
  total_correct = (preds_arr == labels_arr).sum().item()

  total_bin_correct = (preds_arr == np.zeros(len(labels_arr))).sum().item()
  acc_test = running_corrects / total
  bin_acc_test = (total - total_bin_correct)/total
  print("Test Accuracy: %.2f" % acc_test)
  total_probs = []
  conf_matrix = confusion_matrix(labels_arr,preds_arr)
  conf_matrices = multilabel_confusion_matrix(labels_arr,preds_arr,labels=[0,1,2,3])
  print(conf_matrices)
  print(conf_matrix)
  for i in range(0,len(probs_final[1])):
    total_probs.append([probs_final[0][i],probs_final[1][i],probs_final[2][i],probs_final[3][i]])
  for i in range(0,4):
    if i == 0:
      precision, recall, thresholds_precision = precision_recall_curve(labels_arr, probs_final[0], pos_label=0)
      plt.figure()
      plt.plot(recall, precision, color="darkorange", label="Precision-Recall")
      plt.xlabel('Recall')
      plt.ylabel('Precision')
      plt.savefig("recall_precision_classifier%.2f.png" % acc_test)
    fpr, tpr, thresholds = roc_curve(labels_arr, probs_final[i], pos_label=i)
    plt.figure()
    plt.plot(fpr, tpr, color="darkorange", label="Area: ")
    plt.plot([0,1],[0,1],color="navy")
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve for Class CT-%d' % i)
    plt.legend(loc="lower right")
    plt.savefig("roc%d_acc%.2f.png" % (i,acc_test))
  total_tp = 0
  total_fp = 0
  total_tn = 0
  total_fn = 0
  
  stats = conf_matrix.ravel()
  print(stats)
  for i in conf_matrices:
    total_tn = i[0][0]
    total_fp = i[0][1]
    total_fn = i[1][0]
    total_tp = i[1][1]
    print("Precision: %.2f" % (total_tp/(total_tp+total_fp)))
    print("Recall/Sensitivity: %.2f" % (total_tp/(total_tp+total_fn)))
    print("Specificity: %.2f" % (total_tn/(total_tn+total_fp)))
    print("-------")
  score = roc_auc_score(labels_arr, total_probs,multi_class='ovr')
  print("Overall precision: %.2f" % precision_score(labels_arr, preds_arr, labels=[0,1,2,3],average="macro", zero_division=0))
  print("AUC ROC Score: %.2f" % score)
  torch.save(model_ft,"modelacc%.2f.pth" % (acc_test))
if __name__ == "__main__":
  main()